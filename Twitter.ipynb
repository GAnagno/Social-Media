{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.auth.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.api.API object at 0x000001C08CE83080>\n"
     ]
    }
   ],
   "source": [
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " {\n",
      "  \"name\": \"#PSGMUN\",\n",
      "  \"url\": \"http://twitter.com/search?q=%23PSGMUN\",\n",
      "  \"promoted_content\": null,\n",
      "  \"query\": \"%23PSGMUN\",\n",
      "  \"tweet_volume\": 210851\n",
      " },\n",
      " {\n",
      "  \"name\": \"#MUFC\",\n",
      "  \"url\": \"http://twitter.com/search?q=%23MUFC\",\n",
      "  \"promoted_content\": null,\n",
      "  \"query\": \"%23MUFC\",\n",
      "  \"tweet_volume\": 436420\n",
      " }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# The Yahoo! Where On Earth ID for the entire world is 1.\n",
    "# See https://dev.twitter.com/docs/api/1.1/get/trends/place and\n",
    "# http://developer.yahoo.com/geo/geoplanet/\n",
    "\n",
    "WORLD_WOE_ID = 1\n",
    "ZH_WOE_ID = 784794\n",
    "\n",
    "world_trends = api.trends_place(WORLD_WOE_ID)\n",
    "zh_trends = api.trends_place(ZH_WOE_ID)\n",
    "\n",
    "print(json.dumps(list(islice(world_trends[0]['trends'], 2)), indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#ChampionsLeague', 'United', 'Neymar', '#PSGMUFC', '#PSGMUN', '#MUFC'}\n"
     ]
    }
   ],
   "source": [
    "world_trends_set = set([trend['name'] \n",
    "                        for trend in world_trends[0]['trends']])\n",
    "\n",
    "zh_trends_set = set([trend['name'] \n",
    "                     for trend in zh_trends[0]['trends']])\n",
    "\n",
    "common_trends = world_trends_set.intersection(zh_trends_set)\n",
    "\n",
    "print(common_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '#ChampionsLeague' \n",
    "\n",
    "count = 100\n",
    "\n",
    "# Import unquote to prevent url encoding errors in next_results\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# See https://dev.twitter.com/rest/reference/get/search/tweets\n",
    "\n",
    "search_results = api.search(q=q, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = search_results['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['statuses', 'search_metadata'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of statuses 100\n",
      "Length of statuses 200\n",
      "Length of statuses 300\n",
      "Length of statuses 400\n",
      "Length of statuses 500\n"
     ]
    }
   ],
   "source": [
    "# Iterate through 5 more batches of results by following the cursor\n",
    "for _ in range(5):\n",
    "    print('Length of statuses', len(statuses))\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "        \n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "    #print(next_results[1:].split(\"&\"))\n",
    "    search_results = api.search(**kwargs)\n",
    "    statuses += search_results['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one sample search result by slicing the list...\n",
    "#print(json.dumps(statuses[0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
